{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"speed-dating_raw.csv\")\n",
    "x = ['gender', 'race', 'race_o', 'field']\n",
    "columns = list(data)\n",
    "\n",
    "# Deleting bins\n",
    "for column in columns:\n",
    "    if column not in x and data[str(column)].dtype.name == 'object':\n",
    "        del data[str(column)]\n",
    "\n",
    "# Deleting useless columns        \n",
    "del data['has_null']\n",
    "del data['wave']\n",
    "del data['d_age']\n",
    "del data['samerace']\n",
    "del data['expected_happy_with_sd_people']\n",
    "del data['expected_num_interested_in_me']\n",
    "del data['expected_num_matches']\n",
    "del data['like']\n",
    "del data['guess_prob_liked']\n",
    "del data['decision']\n",
    "del data['decision_o']\n",
    "\n",
    "# Replace age NA with mean\n",
    "mean = round(data['age'].mean())\n",
    "data['age'].fillna(mean, inplace = True)\n",
    "mean = round(data['age_o'].mean())\n",
    "data['age_o'].fillna(mean, inplace = True)\n",
    "\n",
    "# Make sure difference in age is correct\n",
    "data['age_d'] = (data['age'] - data['age_o'])\n",
    "data['age_d_abs'] = data['age_d'].abs()\n",
    "\n",
    "# Replace race NA with other\n",
    "data['race'].fillna('other', inplace= True)\n",
    "data['race_o'].fillna('other', inplace = True)\n",
    "\n",
    "# Verifying that same_race is correct with replaced race\n",
    "data['same_race'] = (data['race'] == data['race_o'])\n",
    "\n",
    "# Replace NA with 0 for preferences\n",
    "preferences = ['pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests']          \n",
    "for pref in preferences:\n",
    "    data[pref].fillna(0, inplace = True)\n",
    "\n",
    "# Renaming column names\n",
    "data.rename(columns = {'importance_same_race':'same_race_i',\n",
    "                       'importance_same_religion': 'same_religion_i',\n",
    "                       'pref_o_attractive':'attractive_o_i',\n",
    "                       'pref_o_sincere':'sincere_o_i',\n",
    "                       'pref_o_intelligence':'intelligence_o_i',\n",
    "                       'pref_o_funny':'funny_o_i',\n",
    "                       'pref_o_ambitious':'ambitious_o_i',\n",
    "                       'pref_o_shared_interests':'shared_interests_o_i',\n",
    "                       'attractive_important':'attractive_i',\n",
    "                       'sincere_important': 'sincere_i',\n",
    "                       'intellicence_important': 'intelligence_i',\n",
    "                       'funny_important':'funny_i',\n",
    "                       'ambtition_important':'ambitious_i',\n",
    "                       'shared_interests_important':'shared_interests_i',\n",
    "                       'ambition':'ambitious',\n",
    "                       'sinsere_o': 'sincere_o',\n",
    "                       'ambitous_o':'ambitious_o',\n",
    "                       'ambition_partner':'ambitious_partner'}, inplace = True)\n",
    "\n",
    "# Making sure that opposite's importance columns add up to 100\n",
    "data['o_i'] = data['attractive_o_i'] + data['sincere_o_i'] + data['intelligence_o_i'] + data['funny_o_i'] + data['ambitious_o_i'] + data['shared_interests_o_i']\n",
    "data['attractive_o_i'] = (data['attractive_o_i'] / data['o_i'])\n",
    "data['sincere_o_i'] = (data['sincere_o_i'] / data['o_i'])\n",
    "data['intelligence_o_i'] = (data['intelligence_o_i'] / data['o_i'])\n",
    "data['funny_o_i'] = (data['funny_o_i'] / data['o_i'])\n",
    "data['ambitious_o_i'] = (data['ambitious_o_i'] / data['o_i'])\n",
    "data['shared_interests_o_i'] = (data['shared_interests_o_i'] / data['o_i'])\n",
    "\n",
    "# Making sure that my importance columns add up to 100\n",
    "data['i'] = data['attractive_i'] + data['sincere_i'] + data['intelligence_i'] + data['funny_i'] + data['ambitious_i'] + data['shared_interests_i']\n",
    "data['attractive_i'] = (data['attractive_i'] / data['i'])\n",
    "data['sincere_i'] = (data['sincere_i'] / data['i'])\n",
    "data['intelligence_i'] = (data['intelligence_i'] / data['i'])\n",
    "data['funny_i'] = (data['funny_i'] / data['i'])\n",
    "data['ambitious_i'] = (data['ambitious_i'] / data['i'])\n",
    "data['shared_interests_i'] = (data['shared_interests_i'] / data['i'])\n",
    "\n",
    "del data['o_i']\n",
    "del data['i']\n",
    "\n",
    "# Filling in data that are empty\n",
    "temp = ['attractive_o_i', 'sincere_o_i', 'intelligence_o_i', 'funny_o_i', 'ambitious_o_i', 'shared_interests_o_i', 'attractive_i', 'sincere_i', 'intelligence_i', 'funny_i', 'ambitious_i', 'shared_interests_i']          \n",
    "for t in temp:\n",
    "    data[t].fillna((1.0 / 6.0), inplace = True)\n",
    "\n",
    "# Replacing same_race_i & same_religion_i with mean (importance)\n",
    "mean = data['same_race_i'].mean()\n",
    "data['same_race_i'].fillna(round(mean), inplace = True)\n",
    "\n",
    "mean = data['same_religion_i'].mean()\n",
    "data['same_religion_i'].fillna(round(mean), inplace = True)\n",
    "\n",
    "# One Hot Encoding of categorical data\n",
    "data = pd.concat([data, pd.get_dummies(data['gender'], prefix = 'gender')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['race'], prefix = 'race')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['race_o'], prefix = 'race_o')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['field'], prefix = 'field')], axis = 1)\n",
    "\n",
    "del data['gender']\n",
    "del data['race']\n",
    "del data['race_o']\n",
    "del data['field']\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "data['same_race'] = le.fit_transform(data['same_race'])\n",
    "\n",
    "# Fill NA's with mean\n",
    "mean = data['attractive_o'].mean()\n",
    "data['attractive_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere_o'].mean()\n",
    "data['sincere_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence_o'].mean()\n",
    "data['intelligence_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny_o'].mean()\n",
    "data['funny_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious_o'].mean()\n",
    "data['ambitious_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['shared_interests_o'].mean()\n",
    "data['shared_interests_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['attractive'].mean()\n",
    "data['attractive'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere'].mean()\n",
    "data['sincere'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence'].mean()\n",
    "data['intelligence'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny'].mean()\n",
    "data['funny'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious'].mean()\n",
    "data['ambitious'].fillna(round(mean), inplace = True)\n",
    "mean = data['attractive_partner'].mean()\n",
    "data['attractive_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere_partner'].mean()\n",
    "data['sincere_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence_partner'].mean()\n",
    "data['intelligence_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny_partner'].mean()\n",
    "data['funny_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious_partner'].mean()\n",
    "data['ambitious_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['shared_interests_partner'].mean()\n",
    "data['shared_interests_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['met'].mean()\n",
    "data['met'].fillna(round(mean), inplace = True)\n",
    "\n",
    "# Delete rows with NA's for interests correlate\n",
    "data = data.dropna(axis = 0, subset = ['interests_correlate'])\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between opposite's and my importance\n",
    "data['attractive_i_d'] = (data['attractive_i'] - data['attractive_o_i'])\n",
    "data['sincere_i_d'] = (data['sincere_i'] - data['sincere_o_i'])\n",
    "data['intelligence_i_d'] = (data['intelligence_i'] - data['intelligence_o_i'])\n",
    "data['funny_i_d'] = (data['funny_i'] - data['funny_o_i'])\n",
    "data['ambitious_i_d'] = (data['ambitious_i'] - data['ambitious_o_i'])\n",
    "data['shared_interests_i_d'] = (data['shared_interests_i'] - data['shared_interests_o_i'])\n",
    "\n",
    "# Absolute difference of importance\n",
    "data['attractive_i_d_abs'] = data['attractive_i_d'].abs()\n",
    "data['sincere_i_d_abs'] = data['sincere_i_d'].abs()\n",
    "data['intelligence_i_d_abs'] = data['intelligence_i_d'].abs()\n",
    "data['funny_i_d_abs'] = data['funny_i_d'].abs()\n",
    "data['ambitious_i_d_abs'] = data['ambitious_i_d'].abs()\n",
    "data['shared_interests_i_d_abs'] = data['shared_interests_i_d'].abs()\n",
    "\n",
    "# Difference between opposite's and my rating of me\n",
    "data['attractive_d'] = (data['attractive'] - data['attractive_o'])\n",
    "data['sincere_d'] = (data['sincere'] - data['sincere_o'])\n",
    "data['intelligence_d'] = (data['intelligence'] - data['intelligence_o'])\n",
    "data['funny_d'] = (data['funny'] - data['funny_o'])\n",
    "data['ambitious_d'] = (data['ambitious'] - data['ambitious_o'])\n",
    "data['shared_interests_d'] = (data['shared_interests_partner'] - data['shared_interests_o'])\n",
    "\n",
    "# Absolute difference of rating\n",
    "data['attractive_d_abs'] = data['attractive_d'].abs()\n",
    "data['sincere_d_abs'] = data['sincere_d'].abs()\n",
    "data['intelligence_d_abs'] = data['intelligence_d'].abs()\n",
    "data['funny_d_abs'] = data['funny_d'].abs()\n",
    "data['ambitious_d_abs'] = data['ambitious_d'].abs()\n",
    "data['shared_interests_d_abs'] = data['shared_interests_d'].abs()\n",
    "\n",
    "# Changing from [1-10] scale to percentage for opposite's rating\n",
    "data['o'] = data['attractive_o'] + data['sincere_o'] + data['intelligence_o'] + data['funny_o'] + data['ambitious_o'] + data['shared_interests_o']\n",
    "data['attractive_o_n'] = (data['attractive_o'] / data['o'])\n",
    "data['sincere_o_n'] = (data['sincere_o'] / data['o'])\n",
    "data['intelligence_o_n'] = (data['intelligence_o'] / data['o'])\n",
    "data['funny_o_n'] = (data['funny_o'] / data['o'])\n",
    "data['ambitious_o_n'] = (data['ambitious_o'] / data['o'])\n",
    "data['shared_interests_o_n'] = (data['shared_interests_o'] / data['o'])\n",
    "\n",
    "# Changing from [1-10] scale to percentage for my rating of opposite\n",
    "data['p'] = data['attractive_partner'] + data['sincere_partner'] + data['intelligence_partner'] + data['funny_partner'] + data['ambitious_partner'] + data['shared_interests_partner']\n",
    "data['attractive_p_n'] = (data['attractive_partner'] / data['p'])\n",
    "data['sincere_p_n'] = (data['sincere_partner'] / data['p'])\n",
    "data['intelligence_p_n'] = (data['intelligence_partner'] / data['p'])\n",
    "data['funny_p_n'] = (data['funny_partner'] / data['p'])\n",
    "data['ambitious_p_n'] = (data['ambitious_partner'] / data['p'])\n",
    "data['shared_interests_p_n'] = (data['shared_interests_partner'] / data['p'])\n",
    "\n",
    "del data['o']\n",
    "del data['p']\n",
    "\n",
    "# Filling in blanks with 0\n",
    "preferences = ['attractive_o_n', 'sincere_o_n', 'intelligence_o_n', 'funny_o_n', 'ambitious_o_n', 'shared_interests_o_n', 'attractive_p_n', 'sincere_p_n', 'intelligence_p_n', 'funny_p_n', 'ambitious_p_n', 'shared_interests_p_n']\n",
    "for pref in preferences:\n",
    "    data[pref].fillna(0, inplace = True)\n",
    "\n",
    "# Difference of rating percentage\n",
    "data['d'] = data['attractive_d_abs'] + data['sincere_d_abs'] + data['intelligence_d_abs'] + data['funny_d_abs'] + data['ambitious_d_abs'] + data['shared_interests_d_abs']\n",
    "data['attractive_d_n'] = (data['attractive_d'] / data['d'])\n",
    "data['sincere_d_n'] = (data['sincere_d'] / data['d'])\n",
    "data['intelligence_d_n'] = (data['intelligence_d'] / data['d'])\n",
    "data['funny_d_n'] = (data['funny_d'] / data['d'])\n",
    "data['ambitious_d_n'] = (data['ambitious_d'] / data['d'])\n",
    "data['shared_interests_d_n'] = (data['shared_interests_d'] / data['d'])\n",
    "\n",
    "del data['d']\n",
    "\n",
    "# Absolute difference of rating percentage\n",
    "data['attractive_d_n_abs'] = data['attractive_d_n'].abs()\n",
    "data['sincere_d_n_abs'] = data['sincere_d_n'].abs()\n",
    "data['intelligence_d_n_abs'] = data['intelligence_d_n'].abs()\n",
    "data['funny_d_n_abs'] = data['funny_d_n'].abs()\n",
    "data['ambitious_d_n_abs'] = data['ambitious_d_n'].abs()\n",
    "data['shared_interests_d_n_abs'] = data['shared_interests_d_n'].abs()\n",
    "\n",
    "# Difference between opposite's importance and their rating of me\n",
    "data['attractive_oi_o_d_n'] = (data['attractive_o_i'] - data['attractive_o_n'])\n",
    "data['sincere_oi_o_d_n'] = (data['sincere_o_i'] - data['sincere_o_n'])\n",
    "data['intelligence_oi_o_d_n'] = (data['intelligence_o_i'] - data['intelligence_o_n'])\n",
    "data['funny_oi_o_d_n'] = (data['funny_o_i'] - data['funny_o_n'])\n",
    "data['ambitious_oi_o_d_n'] = (data['ambitious_o_i'] - data['ambitious_o_n'])\n",
    "data['shared_interests_oi_o_d_n'] = (data['shared_interests_o_i'] - data['shared_interests_o_n'])\n",
    "\n",
    "# Absolute difference of opposite's importance and their rating of me\n",
    "data['attractive_oi_o_d_n_abs'] = data['attractive_oi_o_d_n'].abs()\n",
    "data['sincere_oi_o_d_n_abs'] = data['sincere_oi_o_d_n'].abs()\n",
    "data['intelligence_oi_o_d_n_abs'] = data['intelligence_oi_o_d_n'].abs()\n",
    "data['funny_oi_o_d_n_abs'] = data['funny_oi_o_d_n'].abs()\n",
    "data['ambitious_oi_o_d_n_abs'] = data['ambitious_oi_o_d_n'].abs()\n",
    "data['shared_interests_oi_o_d_n_abs'] = data['shared_interests_oi_o_d_n'].abs()\n",
    "\n",
    "# Difference between my importance and my rating of opposite\n",
    "data['attractive_i_p_d_n'] = (data['attractive_i'] - data['attractive_p_n'])\n",
    "data['sincere_i_p_d_n'] = (data['sincere_i'] - data['sincere_p_n'])\n",
    "data['intelligence_i_p_d_n'] = (data['intelligence_i'] - data['intelligence_p_n'])\n",
    "data['funny_i_p_d_n'] = (data['funny_i'] - data['funny_p_n'])\n",
    "data['ambitious_i_p_d_n'] = (data['ambitious_i'] - data['ambitious_p_n'])\n",
    "data['shared_interests_i_p_d_n'] = (data['shared_interests_i'] - data['shared_interests_p_n'])\n",
    "\n",
    "# Absolute difference of my importance and my rating of opposite\n",
    "data['attractive_i_p_d_n_abs'] = data['attractive_i_p_d_n'].abs()\n",
    "data['sincere_i_p_d_n_abs'] = data['sincere_i_p_d_n'].abs()\n",
    "data['intelligence_i_p_d_n_abs'] = data['intelligence_i_p_d_n'].abs()\n",
    "data['funny_i_p_d_n_abs'] = data['funny_i_p_d_n'].abs()\n",
    "data['ambitious_i_p_d_n_abs'] = data['ambitious_i_p_d_n'].abs()\n",
    "data['shared_interests_i_p_d_n_abs'] = data['shared_interests_i_p_d_n'].abs()\n",
    "\n",
    "# Changing from [1-10] scale to percentage for activities\n",
    "data['a'] = data['sports'] + data['tvsports'] + data['exercise'] + data['dining'] + data['museums'] + data['art'] + data['hiking'] + data['gaming'] + data['clubbing'] + data['reading'] + data['tv'] + data['theater'] + data['movies'] + data['concerts'] + data['music'] + data['shopping'] + data['yoga']\n",
    "data['sports_n'] = (data['sports'] / data['a']) \n",
    "data['tvsports_n'] = (data['tvsports'] / data['a']) \n",
    "data['exercise_n'] = (data['exercise'] / data['a']) \n",
    "data['dining_n'] = (data['dining'] / data['a']) \n",
    "data['museums_n'] = (data['museums'] / data['a']) \n",
    "data['art_n'] = (data['art'] / data['a']) \n",
    "data['hiking_n'] = (data['hiking'] / data['a']) \n",
    "data['gaming_n'] = (data['gaming'] / data['a']) \n",
    "data['clubbing_n'] = (data['clubbing'] / data['a']) \n",
    "data['reading_n'] = (data['reading'] / data['a']) \n",
    "data['tv_n'] = (data['tv'] / data['a']) \n",
    "data['theater_n'] = (data['theater'] / data['a']) \n",
    "data['movies_n'] = (data['movies'] / data['a']) \n",
    "data['concerts_n'] = (data['concerts'] / data['a']) \n",
    "data['music_n'] = (data['music'] / data['a']) \n",
    "data['shopping_n'] = (data['shopping'] / data['a']) \n",
    "data['yoga_n'] = (data['yoga'] / data['a']) \n",
    "\n",
    "del data['a']\n",
    "\n",
    "data.to_csv('Speed_Dating_Clean_noSMOTE.csv', index = False)\n",
    "\n",
    "# Create train and test data for Darwin (no SMOTE)\n",
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.20, random_state = 11)\n",
    "train_data = pd.concat([train_X, train_Y], axis = 1)\n",
    "train_data.to_csv('Speed_Dating_Clean_noSMOTE_train.csv', index = False)\n",
    "test_data = pd.concat([test_X, test_Y], axis = 1)\n",
    "test_data.to_csv('Speed_Dating_Clean_noSMOTE_test.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "sm = SMOTE(sampling_strategy = 'minority', random_state = 11)\n",
    "smote_X, smote_Y = sm.fit_sample(data_X, data_Y)\n",
    "\n",
    "print(smote_X.shape)\n",
    "print(smote_Y.shape)\n",
    "smote_X = DataFrame(smote_X, columns = data_X.columns)\n",
    "smote_Y = Series(smote_Y, name = 'match')\n",
    "data = pd.concat([smote_X, smote_Y], axis = 1)\n",
    "data.head()\n",
    "\n",
    "data.to_csv('Speed_Dating_Clean_SMOTE.csv', index = False)\n",
    "\n",
    "# Create train and test data for Darwin (SMOTE)\n",
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.20, random_state = 11)\n",
    "train_data = pd.concat([train_X, train_Y], axis = 1)\n",
    "train_data.to_csv('Speed_Dating_Clean_SMOTE_train.csv', index = False)\n",
    "test_data = pd.concat([test_X, test_Y], axis = 1)\n",
    "test_data.to_csv('Speed_Dating_Clean_SMOTE_test.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "print(data_X.shape)\n",
    "data_X.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = StandardScaler().fit_transform(data_X)\n",
    "\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "pca_X = pca.fit_transform(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_X.shape)\n",
    "pd.DataFrame(pca_X).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state = 11)\n",
    "\n",
    "# Set parameters\n",
    "params = {\"criterion\" : ['gini', 'entropy'],\n",
    "          \"max_depth\": [5, 10, 15, 20],\n",
    "          \"max_features\": ['sqrt', 'log2'],\n",
    "          \"min_samples_leaf\": [5, 10, 15, 20]\n",
    "         }\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(clf, params, cv = 10)\n",
    "grid_search.fit(pca_X, data_Y)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(pca_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 11)\n",
    "\n",
    "# Set parameters\n",
    "params = {\"criterion\" : ['gini', 'entropy'],\n",
    "          \"max_depth\": [5, 10, 15, 20],\n",
    "          \"max_features\": ['sqrt', 'log2'],\n",
    "          \"min_samples_leaf\": [5, 10, 15, 20]\n",
    "         }\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(clf, params, cv = 5)\n",
    "grid_search.fit(pca_X, data_Y)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(pca_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = AdaBoostClassifier(random_state = 11)\n",
    "\n",
    "# Set parameters\n",
    "params = {\"n_estimators\": list(range(50, 150, 10))}\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(clf, params, cv = 5)\n",
    "grid_search.fit(pca_X, data_Y)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(pca_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 11)\n",
    "\n",
    "# Set parameters\n",
    "params = {\"n_estimators\": list(range(100, 200, 10))}\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(clf, params, cv = 5)\n",
    "grid_search.fit(pca_X, data_Y)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(pca_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_scaled_X = MinMaxScaler().fit_transform(data_X)\n",
    "\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "knn_X = pca.fit_transform(knn_scaled_X)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Set parameters\n",
    "params = {'n_neighbors': list(range(1, 50, 2))}\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(knn, params, cv = 5)\n",
    "grid_search.fit(knn_X, data_Y)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(knn_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Creating svc. Here we use the default instead of linear as mentioned on the piazza post. \n",
    "svc_clf = SVC(random_state = 11)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('svc', svc_clf)])\n",
    "\n",
    "# Pass the pipeline in to a cross_val_score \n",
    "scores = cross_val_score(pipe, data_X, data_Y)\n",
    "\n",
    "# Printing the average accuracy\n",
    "print('Average Accuracy:', scores.mean() * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(data_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create NB clf and fit it\n",
    "clf_rf = GaussianNB()\n",
    "\n",
    "# Cross validation\n",
    "scores = cross_val_score(clf_rf, data_X, data_Y, cv = 10)                                         \n",
    "print(\"Accuracy with 10-fold cross validation:\", scores.mean() * 100)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "pred_Y = cross_val_predict(clf_rf, data_X, data_Y, cv = 10)\n",
    "print('The confusion matrix is:\\n', confusion_matrix(data_Y, pred_Y))\n",
    "print('\\n', classification_report(data_Y, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = MLPClassifier(random_state = 11)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline(steps = [('scaler', scaler), ('clf', clf)])\n",
    "\n",
    "# Set parameters\n",
    "param_grid = {\n",
    "    'clf__hidden_layer_sizes': [(10,), (20,), (30,), (40,), (50,), (60,), (70,), (80,), (90,), (100,)],\n",
    "    'clf__activation': ['identity', 'logistic', 'tanh', 'relu']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, iid = False, cv = 5)\n",
    "\n",
    "# Fit data and print results\n",
    "grid_search.fit(data_X, data_Y)\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\", classification_report(data_Y, grid_search.predict(data_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('EMMIECHNG@UTEXAS.EDU', 'QMCZgepW6u')\n",
    "\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = 'Speed_Dating_Clean_SMOTE_train.csv'\n",
    "dataset_test = 'Speed_Dating_Clean_SMOTE_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload train dataset to Darwin\n",
    "data_SMOTE_train = pd.read_csv(\"Speed_Dating_Clean_SMOTE_train.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_SMOTE_train.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test dataset to Darwin\n",
    "data_SMOTE_test = pd.read_csv(\"Speed_Dating_Clean_SMOTE_test.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_SMOTE_test.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_train, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean test dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_test, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Darwin model\n",
    "model = target + \"_model\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = dataset_train,\n",
    "                                 model_name = model,\n",
    "                                 max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra training for Darwin model\n",
    "status, job_id = ds.resume_training_model(dataset_names = dataset_train, \n",
    "                                         model_name = model,\n",
    "                                         max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model decided by Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Darwin used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze DeepNeuralNetwork model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin DeepNeuralNetwork model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze RandomForest model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin RandomForest model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze GradientBoosted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin GradientBoosted model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin (noSMOTE) : imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('EMMIECHNG@UTEXAS.EDU', 'QMCZgepW6u')\n",
    "\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = 'Speed_Dating_Clean_noSMOTE_train.csv'\n",
    "dataset_test = 'Speed_Dating_Clean_noSMOTE_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload train dataset to Darwin\n",
    "data_noSMOTE_train = pd.read_csv(\"Speed_Dating_Clean_noSMOTE_train.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_noSMOTE_train.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test dataset to Darwin\n",
    "data_noSMOTE_test = pd.read_csv(\"Speed_Dating_Clean_noSMOTE_test.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_noSMOTE_test.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_train, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean test dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_test, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Darwin model\n",
    "model = target + \"_model\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = dataset_train,\n",
    "                                 model_name = model,\n",
    "                                 max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra training for Darwin model\n",
    "status, job_id = ds.resume_training_model(dataset_names = dataset_train, \n",
    "                                         model_name = model,\n",
    "                                         max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model decided by Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Darwin used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze DeepNeuralNetwork model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin DeepNeuralNetwork model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze RandomForest model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin RandomForest model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze GradientBoosted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin GradientBoosted model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_name, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('\\nR^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
