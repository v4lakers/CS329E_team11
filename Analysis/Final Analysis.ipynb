{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8220, 287)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>same_race_i</th>\n",
       "      <th>same_religion_i</th>\n",
       "      <th>attractive_o_i</th>\n",
       "      <th>sincere_o_i</th>\n",
       "      <th>intelligence_o_i</th>\n",
       "      <th>funny_o_i</th>\n",
       "      <th>ambitious_o_i</th>\n",
       "      <th>shared_interests_o_i</th>\n",
       "      <th>...</th>\n",
       "      <th>field_tc [health ed]</th>\n",
       "      <th>field_teaching of english</th>\n",
       "      <th>field_tesol</th>\n",
       "      <th>field_theater</th>\n",
       "      <th>field_theatre management &amp; producing</th>\n",
       "      <th>field_theory</th>\n",
       "      <th>field_undergrad - gs</th>\n",
       "      <th>field_urban planning</th>\n",
       "      <th>field_working</th>\n",
       "      <th>field_writing: literary nonfiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  age_o  same_race_i  same_religion_i  attractive_o_i  sincere_o_i  \\\n",
       "0  21.0   27.0          2.0              4.0            0.35         0.20   \n",
       "1  21.0   22.0          2.0              4.0            0.60         0.00   \n",
       "2  21.0   22.0          2.0              4.0            0.19         0.18   \n",
       "3  21.0   23.0          2.0              4.0            0.30         0.05   \n",
       "4  21.0   24.0          2.0              4.0            0.30         0.10   \n",
       "\n",
       "   intelligence_o_i  funny_o_i  ambitious_o_i  shared_interests_o_i  \\\n",
       "0              0.20       0.20           0.00                  0.05   \n",
       "1              0.00       0.40           0.00                  0.00   \n",
       "2              0.19       0.18           0.14                  0.12   \n",
       "3              0.15       0.40           0.05                  0.05   \n",
       "4              0.20       0.10           0.10                  0.20   \n",
       "\n",
       "                  ...                  field_tc [health ed]  \\\n",
       "0                 ...                                     0   \n",
       "1                 ...                                     0   \n",
       "2                 ...                                     0   \n",
       "3                 ...                                     0   \n",
       "4                 ...                                     0   \n",
       "\n",
       "   field_teaching of english  field_tesol  field_theater  \\\n",
       "0                          0            0              0   \n",
       "1                          0            0              0   \n",
       "2                          0            0              0   \n",
       "3                          0            0              0   \n",
       "4                          0            0              0   \n",
       "\n",
       "   field_theatre management & producing  field_theory  field_undergrad - gs  \\\n",
       "0                                     0             0                     0   \n",
       "1                                     0             0                     0   \n",
       "2                                     0             0                     0   \n",
       "3                                     0             0                     0   \n",
       "4                                     0             0                     0   \n",
       "\n",
       "   field_urban planning  field_working  field_writing: literary nonfiction  \n",
       "0                     0              0                                   0  \n",
       "1                     0              0                                   0  \n",
       "2                     0              0                                   0  \n",
       "3                     0              0                                   0  \n",
       "4                     0              0                                   0  \n",
       "\n",
       "[5 rows x 287 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"speed-dating_raw.csv\")\n",
    "x = ['gender', 'race', 'race_o', 'field']\n",
    "columns = list(data)\n",
    "\n",
    "# Deleting bins\n",
    "for column in columns:\n",
    "    if column not in x and data[str(column)].dtype.name == 'object':\n",
    "        del data[str(column)]\n",
    "\n",
    "# Deleting useless columns        \n",
    "del data['has_null']\n",
    "del data['wave']\n",
    "del data['d_age']\n",
    "del data['samerace']\n",
    "del data['expected_happy_with_sd_people']\n",
    "del data['expected_num_interested_in_me']\n",
    "del data['expected_num_matches']\n",
    "del data['like']\n",
    "del data['guess_prob_liked']\n",
    "del data['decision']\n",
    "del data['decision_o']\n",
    "\n",
    "# Replace age NA with mean\n",
    "mean = round(data['age'].mean())\n",
    "data['age'].fillna(mean, inplace = True)\n",
    "mean = round(data['age_o'].mean())\n",
    "data['age_o'].fillna(mean, inplace = True)\n",
    "\n",
    "# Make sure difference in age is correct\n",
    "data['age_d'] = (data['age'] - data['age_o'])\n",
    "data['age_d_abs'] = data['age_d'].abs()\n",
    "\n",
    "# Replace race NA with other\n",
    "data['race'].fillna('other', inplace= True)\n",
    "data['race_o'].fillna('other', inplace = True)\n",
    "\n",
    "# Verifying that same_race is correct with replaced race\n",
    "data['same_race'] = (data['race'] == data['race_o'])\n",
    "\n",
    "# Replace NA with 0 for preferences\n",
    "preferences = ['pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests']          \n",
    "for pref in preferences:\n",
    "    data[pref].fillna(0, inplace = True)\n",
    "\n",
    "# Renaming column names\n",
    "data.rename(columns = {'importance_same_race':'same_race_i',\n",
    "                       'importance_same_religion': 'same_religion_i',\n",
    "                       'pref_o_attractive':'attractive_o_i',\n",
    "                       'pref_o_sincere':'sincere_o_i',\n",
    "                       'pref_o_intelligence':'intelligence_o_i',\n",
    "                       'pref_o_funny':'funny_o_i',\n",
    "                       'pref_o_ambitious':'ambitious_o_i',\n",
    "                       'pref_o_shared_interests':'shared_interests_o_i',\n",
    "                       'attractive_important':'attractive_i',\n",
    "                       'sincere_important': 'sincere_i',\n",
    "                       'intellicence_important': 'intelligence_i',\n",
    "                       'funny_important':'funny_i',\n",
    "                       'ambtition_important':'ambitious_i',\n",
    "                       'shared_interests_important':'shared_interests_i',\n",
    "                       'ambition':'ambitious',\n",
    "                       'sinsere_o': 'sincere_o',\n",
    "                       'ambitous_o':'ambitious_o',\n",
    "                       'ambition_partner':'ambitious_partner'}, inplace = True)\n",
    "\n",
    "# Making sure that opposite's importance columns add up to 100\n",
    "data['o_i'] = data['attractive_o_i'] + data['sincere_o_i'] + data['intelligence_o_i'] + data['funny_o_i'] + data['ambitious_o_i'] + data['shared_interests_o_i']\n",
    "data['attractive_o_i'] = (data['attractive_o_i'] / data['o_i'])\n",
    "data['sincere_o_i'] = (data['sincere_o_i'] / data['o_i'])\n",
    "data['intelligence_o_i'] = (data['intelligence_o_i'] / data['o_i'])\n",
    "data['funny_o_i'] = (data['funny_o_i'] / data['o_i'])\n",
    "data['ambitious_o_i'] = (data['ambitious_o_i'] / data['o_i'])\n",
    "data['shared_interests_o_i'] = (data['shared_interests_o_i'] / data['o_i'])\n",
    "\n",
    "# Making sure that my importance columns add up to 100\n",
    "data['i'] = data['attractive_i'] + data['sincere_i'] + data['intelligence_i'] + data['funny_i'] + data['ambitious_i'] + data['shared_interests_i']\n",
    "data['attractive_i'] = (data['attractive_i'] / data['i'])\n",
    "data['sincere_i'] = (data['sincere_i'] / data['i'])\n",
    "data['intelligence_i'] = (data['intelligence_i'] / data['i'])\n",
    "data['funny_i'] = (data['funny_i'] / data['i'])\n",
    "data['ambitious_i'] = (data['ambitious_i'] / data['i'])\n",
    "data['shared_interests_i'] = (data['shared_interests_i'] / data['i'])\n",
    "\n",
    "del data['o_i']\n",
    "del data['i']\n",
    "\n",
    "# Filling in data that are empty\n",
    "temp = ['attractive_o_i', 'sincere_o_i', 'intelligence_o_i', 'funny_o_i', 'ambitious_o_i', 'shared_interests_o_i', 'attractive_i', 'sincere_i', 'intelligence_i', 'funny_i', 'ambitious_i', 'shared_interests_i']          \n",
    "for t in temp:\n",
    "    data[t].fillna((1.0 / 6.0), inplace = True)\n",
    "\n",
    "# Replacing same_race_i & same_religion_i with mean (importance)\n",
    "mean = data['same_race_i'].mean()\n",
    "data['same_race_i'].fillna(round(mean), inplace = True)\n",
    "\n",
    "mean = data['same_religion_i'].mean()\n",
    "data['same_religion_i'].fillna(round(mean), inplace = True)\n",
    "\n",
    "# One Hot Encoding of categorical data\n",
    "data = pd.concat([data, pd.get_dummies(data['gender'], prefix = 'gender')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['race'], prefix = 'race')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['race_o'], prefix = 'race_o')], axis = 1)\n",
    "data = pd.concat([data, pd.get_dummies(data['field'], prefix = 'field')], axis = 1)\n",
    "\n",
    "del data['gender']\n",
    "del data['race']\n",
    "del data['race_o']\n",
    "del data['field']\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "data['same_race'] = le.fit_transform(data['same_race'])\n",
    "\n",
    "# Fill NA's with mean\n",
    "mean = data['attractive_o'].mean()\n",
    "data['attractive_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere_o'].mean()\n",
    "data['sincere_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence_o'].mean()\n",
    "data['intelligence_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny_o'].mean()\n",
    "data['funny_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious_o'].mean()\n",
    "data['ambitious_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['shared_interests_o'].mean()\n",
    "data['shared_interests_o'].fillna(round(mean), inplace = True)\n",
    "mean = data['attractive'].mean()\n",
    "data['attractive'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere'].mean()\n",
    "data['sincere'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence'].mean()\n",
    "data['intelligence'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny'].mean()\n",
    "data['funny'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious'].mean()\n",
    "data['ambitious'].fillna(round(mean), inplace = True)\n",
    "mean = data['attractive_partner'].mean()\n",
    "data['attractive_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['sincere_partner'].mean()\n",
    "data['sincere_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['intelligence_partner'].mean()\n",
    "data['intelligence_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['funny_partner'].mean()\n",
    "data['funny_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['ambitious_partner'].mean()\n",
    "data['ambitious_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['shared_interests_partner'].mean()\n",
    "data['shared_interests_partner'].fillna(round(mean), inplace = True)\n",
    "mean = data['met'].mean()\n",
    "data['met'].fillna(round(mean), inplace = True)\n",
    "\n",
    "# Delete rows with NA's for interests correlate\n",
    "data = data.dropna(axis = 0, subset = ['interests_correlate'])\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between opposite's and my importance\n",
    "data['attractive_i_d'] = (data['attractive_i'] - data['attractive_o_i'])\n",
    "data['sincere_i_d'] = (data['sincere_i'] - data['sincere_o_i'])\n",
    "data['intelligence_i_d'] = (data['intelligence_i'] - data['intelligence_o_i'])\n",
    "data['funny_i_d'] = (data['funny_i'] - data['funny_o_i'])\n",
    "data['ambitious_i_d'] = (data['ambitious_i'] - data['ambitious_o_i'])\n",
    "data['shared_interests_i_d'] = (data['shared_interests_i'] - data['shared_interests_o_i'])\n",
    "\n",
    "# Absolute difference of importance\n",
    "data['attractive_i_d_abs'] = data['attractive_i_d'].abs()\n",
    "data['sincere_i_d_abs'] = data['sincere_i_d'].abs()\n",
    "data['intelligence_i_d_abs'] = data['intelligence_i_d'].abs()\n",
    "data['funny_i_d_abs'] = data['funny_i_d'].abs()\n",
    "data['ambitious_i_d_abs'] = data['ambitious_i_d'].abs()\n",
    "data['shared_interests_i_d_abs'] = data['shared_interests_i_d'].abs()\n",
    "\n",
    "# Difference between opposite's and my rating of me\n",
    "data['attractive_d'] = (data['attractive'] - data['attractive_o'])\n",
    "data['sincere_d'] = (data['sincere'] - data['sincere_o'])\n",
    "data['intelligence_d'] = (data['intelligence'] - data['intelligence_o'])\n",
    "data['funny_d'] = (data['funny'] - data['funny_o'])\n",
    "data['ambitious_d'] = (data['ambitious'] - data['ambitious_o'])\n",
    "data['shared_interests_d'] = (data['shared_interests_partner'] - data['shared_interests_o'])\n",
    "\n",
    "# Absolute difference of rating\n",
    "data['attractive_d_abs'] = data['attractive_d'].abs()\n",
    "data['sincere_d_abs'] = data['sincere_d'].abs()\n",
    "data['intelligence_d_abs'] = data['intelligence_d'].abs()\n",
    "data['funny_d_abs'] = data['funny_d'].abs()\n",
    "data['ambitious_d_abs'] = data['ambitious_d'].abs()\n",
    "data['shared_interests_d_abs'] = data['shared_interests_d'].abs()\n",
    "\n",
    "# Changing from [1-10] scale to percentage for opposite's rating\n",
    "data['o'] = data['attractive_o'] + data['sincere_o'] + data['intelligence_o'] + data['funny_o'] + data['ambitious_o'] + data['shared_interests_o']\n",
    "data['attractive_o_n'] = (data['attractive_o'] / data['o'])\n",
    "data['sincere_o_n'] = (data['sincere_o'] / data['o'])\n",
    "data['intelligence_o_n'] = (data['intelligence_o'] / data['o'])\n",
    "data['funny_o_n'] = (data['funny_o'] / data['o'])\n",
    "data['ambitious_o_n'] = (data['ambitious_o'] / data['o'])\n",
    "data['shared_interests_o_n'] = (data['shared_interests_o'] / data['o'])\n",
    "\n",
    "# Changing from [1-10] scale to percentage for my rating of opposite\n",
    "data['p'] = data['attractive_partner'] + data['sincere_partner'] + data['intelligence_partner'] + data['funny_partner'] + data['ambitious_partner'] + data['shared_interests_partner']\n",
    "data['attractive_p_n'] = (data['attractive_partner'] / data['p'])\n",
    "data['sincere_p_n'] = (data['sincere_partner'] / data['p'])\n",
    "data['intelligence_p_n'] = (data['intelligence_partner'] / data['p'])\n",
    "data['funny_p_n'] = (data['funny_partner'] / data['p'])\n",
    "data['ambitious_p_n'] = (data['ambitious_partner'] / data['p'])\n",
    "data['shared_interests_p_n'] = (data['shared_interests_partner'] / data['p'])\n",
    "\n",
    "del data['o']\n",
    "del data['p']\n",
    "\n",
    "# Filling in blanks with 0\n",
    "preferences = ['attractive_o_n', 'sincere_o_n', 'intelligence_o_n', 'funny_o_n', 'ambitious_o_n', 'shared_interests_o_n', 'attractive_p_n', 'sincere_p_n', 'intelligence_p_n', 'funny_p_n', 'ambitious_p_n', 'shared_interests_p_n']\n",
    "for pref in preferences:\n",
    "    data[pref].fillna(0, inplace = True)\n",
    "\n",
    "# Difference of rating percentage\n",
    "data['d'] = data['attractive_d_abs'] + data['sincere_d_abs'] + data['intelligence_d_abs'] + data['funny_d_abs'] + data['ambitious_d_abs'] + data['shared_interests_d_abs']\n",
    "data['attractive_d_n'] = (data['attractive_d'] / data['d'])\n",
    "data['sincere_d_n'] = (data['sincere_d'] / data['d'])\n",
    "data['intelligence_d_n'] = (data['intelligence_d'] / data['d'])\n",
    "data['funny_d_n'] = (data['funny_d'] / data['d'])\n",
    "data['ambitious_d_n'] = (data['ambitious_d'] / data['d'])\n",
    "data['shared_interests_d_n'] = (data['shared_interests_d'] / data['d'])\n",
    "\n",
    "del data['d']\n",
    "\n",
    "# Absolute difference of rating percentage\n",
    "data['attractive_d_n_abs'] = data['attractive_d_n'].abs()\n",
    "data['sincere_d_n_abs'] = data['sincere_d_n'].abs()\n",
    "data['intelligence_d_n_abs'] = data['intelligence_d_n'].abs()\n",
    "data['funny_d_n_abs'] = data['funny_d_n'].abs()\n",
    "data['ambitious_d_n_abs'] = data['ambitious_d_n'].abs()\n",
    "data['shared_interests_d_n_abs'] = data['shared_interests_d_n'].abs()\n",
    "\n",
    "# Difference between opposite's importance and their rating of me\n",
    "data['attractive_oi_o_d_n'] = (data['attractive_o_i'] - data['attractive_o_n'])\n",
    "data['sincere_oi_o_d_n'] = (data['sincere_o_i'] - data['sincere_o_n'])\n",
    "data['intelligence_oi_o_d_n'] = (data['intelligence_o_i'] - data['intelligence_o_n'])\n",
    "data['funny_oi_o_d_n'] = (data['funny_o_i'] - data['funny_o_n'])\n",
    "data['ambitious_oi_o_d_n'] = (data['ambitious_o_i'] - data['ambitious_o_n'])\n",
    "data['shared_interests_oi_o_d_n'] = (data['shared_interests_o_i'] - data['shared_interests_o_n'])\n",
    "\n",
    "# Absolute difference of opposite's importance and their rating of me\n",
    "data['attractive_oi_o_d_n_abs'] = data['attractive_oi_o_d_n'].abs()\n",
    "data['sincere_oi_o_d_n_abs'] = data['sincere_oi_o_d_n'].abs()\n",
    "data['intelligence_oi_o_d_n_abs'] = data['intelligence_oi_o_d_n'].abs()\n",
    "data['funny_oi_o_d_n_abs'] = data['funny_oi_o_d_n'].abs()\n",
    "data['ambitious_oi_o_d_n_abs'] = data['ambitious_oi_o_d_n'].abs()\n",
    "data['shared_interests_oi_o_d_n_abs'] = data['shared_interests_oi_o_d_n'].abs()\n",
    "\n",
    "# Difference between my importance and my rating of opposite\n",
    "data['attractive_i_p_d_n'] = (data['attractive_i'] - data['attractive_p_n'])\n",
    "data['sincere_i_p_d_n'] = (data['sincere_i'] - data['sincere_p_n'])\n",
    "data['intelligence_i_p_d_n'] = (data['intelligence_i'] - data['intelligence_p_n'])\n",
    "data['funny_i_p_d_n'] = (data['funny_i'] - data['funny_p_n'])\n",
    "data['ambitious_i_p_d_n'] = (data['ambitious_i'] - data['ambitious_p_n'])\n",
    "data['shared_interests_i_p_d_n'] = (data['shared_interests_i'] - data['shared_interests_p_n'])\n",
    "\n",
    "# Absolute difference of my importance and my rating of opposite\n",
    "data['attractive_i_p_d_n_abs'] = data['attractive_i_p_d_n'].abs()\n",
    "data['sincere_i_p_d_n_abs'] = data['sincere_i_p_d_n'].abs()\n",
    "data['intelligence_i_p_d_n_abs'] = data['intelligence_i_p_d_n'].abs()\n",
    "data['funny_i_p_d_n_abs'] = data['funny_i_p_d_n'].abs()\n",
    "data['ambitious_i_p_d_n_abs'] = data['ambitious_i_p_d_n'].abs()\n",
    "data['shared_interests_i_p_d_n_abs'] = data['shared_interests_i_p_d_n'].abs()\n",
    "\n",
    "# Changing from [1-10] scale to percentage for activities\n",
    "data['a'] = data['sports'] + data['tvsports'] + data['exercise'] + data['dining'] + data['museums'] + data['art'] + data['hiking'] + data['gaming'] + data['clubbing'] + data['reading'] + data['tv'] + data['theater'] + data['movies'] + data['concerts'] + data['music'] + data['shopping'] + data['yoga']\n",
    "data['sports_n'] = (data['sports'] / data['a']) \n",
    "data['tvsports_n'] = (data['tvsports'] / data['a']) \n",
    "data['exercise_n'] = (data['exercise'] / data['a']) \n",
    "data['dining_n'] = (data['dining'] / data['a']) \n",
    "data['museums_n'] = (data['museums'] / data['a']) \n",
    "data['art_n'] = (data['art'] / data['a']) \n",
    "data['hiking_n'] = (data['hiking'] / data['a']) \n",
    "data['gaming_n'] = (data['gaming'] / data['a']) \n",
    "data['clubbing_n'] = (data['clubbing'] / data['a']) \n",
    "data['reading_n'] = (data['reading'] / data['a']) \n",
    "data['tv_n'] = (data['tv'] / data['a']) \n",
    "data['theater_n'] = (data['theater'] / data['a']) \n",
    "data['movies_n'] = (data['movies'] / data['a']) \n",
    "data['concerts_n'] = (data['concerts'] / data['a']) \n",
    "data['music_n'] = (data['music'] / data['a']) \n",
    "data['shopping_n'] = (data['shopping'] / data['a']) \n",
    "data['yoga_n'] = (data['yoga'] / data['a']) \n",
    "\n",
    "del data['a']\n",
    "\n",
    "data.to_csv('Speed_Dating_Clean_noSMOTE.csv', index = False)\n",
    "\n",
    "# Create train and test data for Darwin (no SMOTE)\n",
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.20, random_state = 11)\n",
    "train_data = pd.concat([train_X, train_Y], axis = 1)\n",
    "train_data.to_csv('Speed_Dating_Clean_noSMOTE_train.csv', index = False)\n",
    "test_data = pd.concat([test_X, test_Y], axis = 1)\n",
    "test_data.to_csv('Speed_Dating_Clean_noSMOTE_test.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13740, 375)\n",
      "(13740,)\n"
     ]
    }
   ],
   "source": [
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "sm = SMOTE(sampling_strategy = 'minority', random_state = 11)\n",
    "smote_X, smote_Y = sm.fit_sample(data_X, data_Y)\n",
    "\n",
    "print(smote_X.shape)\n",
    "print(smote_Y.shape)\n",
    "smote_X = DataFrame(smote_X, columns = data_X.columns)\n",
    "smote_Y = Series(smote_Y, name = 'match')\n",
    "data = pd.concat([smote_X, smote_Y], axis = 1)\n",
    "data.head()\n",
    "\n",
    "data.to_csv('Speed_Dating_Clean_SMOTE.csv', index = False)\n",
    "\n",
    "# Create train and test data for Darwin (SMOTE)\n",
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.20, random_state = 11)\n",
    "train_data = pd.concat([train_X, train_Y], axis = 1)\n",
    "train_data.to_csv('Speed_Dating_Clean_SMOTE_train.csv', index = False)\n",
    "test_data = pd.concat([test_X, test_Y], axis = 1)\n",
    "test_data.to_csv('Speed_Dating_Clean_SMOTE_test.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13740, 375)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>same_race_i</th>\n",
       "      <th>same_religion_i</th>\n",
       "      <th>attractive_o_i</th>\n",
       "      <th>sincere_o_i</th>\n",
       "      <th>intelligence_o_i</th>\n",
       "      <th>funny_o_i</th>\n",
       "      <th>ambitious_o_i</th>\n",
       "      <th>shared_interests_o_i</th>\n",
       "      <th>...</th>\n",
       "      <th>gaming_n</th>\n",
       "      <th>clubbing_n</th>\n",
       "      <th>reading_n</th>\n",
       "      <th>tv_n</th>\n",
       "      <th>theater_n</th>\n",
       "      <th>movies_n</th>\n",
       "      <th>concerts_n</th>\n",
       "      <th>music_n</th>\n",
       "      <th>shopping_n</th>\n",
       "      <th>yoga_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  age_o  same_race_i  same_religion_i  attractive_o_i  sincere_o_i  \\\n",
       "0  21.0   27.0          2.0              4.0            0.35         0.20   \n",
       "1  21.0   22.0          2.0              4.0            0.60         0.00   \n",
       "2  21.0   22.0          2.0              4.0            0.19         0.18   \n",
       "3  21.0   23.0          2.0              4.0            0.30         0.05   \n",
       "4  21.0   24.0          2.0              4.0            0.30         0.10   \n",
       "\n",
       "   intelligence_o_i  funny_o_i  ambitious_o_i  shared_interests_o_i    ...     \\\n",
       "0              0.20       0.20           0.00                  0.05    ...      \n",
       "1              0.00       0.40           0.00                  0.00    ...      \n",
       "2              0.19       0.18           0.14                  0.12    ...      \n",
       "3              0.15       0.40           0.05                  0.05    ...      \n",
       "4              0.20       0.10           0.10                  0.20    ...      \n",
       "\n",
       "   gaming_n  clubbing_n  reading_n      tv_n  theater_n  movies_n  concerts_n  \\\n",
       "0  0.010526    0.052632   0.063158  0.094737   0.010526  0.105263    0.105263   \n",
       "1  0.010526    0.052632   0.063158  0.094737   0.010526  0.105263    0.105263   \n",
       "2  0.010526    0.052632   0.063158  0.094737   0.010526  0.105263    0.105263   \n",
       "3  0.010526    0.052632   0.063158  0.094737   0.010526  0.105263    0.105263   \n",
       "4  0.010526    0.052632   0.063158  0.094737   0.010526  0.105263    0.105263   \n",
       "\n",
       "    music_n  shopping_n    yoga_n  \n",
       "0  0.094737    0.084211  0.010526  \n",
       "1  0.094737    0.084211  0.010526  \n",
       "2  0.094737    0.084211  0.010526  \n",
       "3  0.094737    0.084211  0.010526  \n",
       "4  0.094737    0.084211  0.010526  \n",
       "\n",
       "[5 rows x 375 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y = data['match']\n",
    "data_X = data.drop(['match'], axis = 1)\n",
    "print(data_X.shape)\n",
    "data_X.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__criterion': 'entropy', 'clf__max_depth': 30, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 5}\n",
      "Accuracy on Train Data is : 77.56550218340611\n",
      "Accuracy on Test data is: 0.7783842794759825\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78      1377\n",
      "           1       0.77      0.79      0.78      1371\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2748\n",
      "   macro avg       0.78      0.78      0.78      2748\n",
      "weighted avg       0.78      0.78      0.78      2748\n",
      "\n",
      "CPU times: user 10min 25s, sys: 17.6 s, total: 10min 43s\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "clf = DecisionTreeClassifier(random_state = 11)\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "# Set parameters\n",
    "params = {\"clf__criterion\" : ['gini', 'entropy'],\n",
    "          \"clf__max_depth\": [15, 20, 25, 30, 35],\n",
    "          \"clf__max_features\": ['sqrt', 'log2'],\n",
    "          \"clf__min_samples_leaf\": [5, 10, 15]\n",
    "         }\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(pipe, params, cv = 10)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy on Train Data is :\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print Test Accuracy\n",
    "preds = grid_search.predict(X_test)\n",
    "print('Accuracy on Test data is:', (accuracy_score(Y_test, preds))*100)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Test Data\")\n",
    "print(\"\\n\", classification_report(Y_test, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "clf = RandomForestClassifier(random_state = 11, n_estimators = 100)\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "# Set parameters\n",
    "params = {\"clf__criterion\" : ['gini', 'entropy'],\n",
    "          \"clf__max_depth\": [15, 20, 25, 30, 35],\n",
    "          \"clf__max_features\": ['sqrt', 'log2'],\n",
    "          \"clf__min_samples_leaf\": [5, 10, 15]\n",
    "         }\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(pipe, params, cv = 10)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy on Train Data is :\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print Test Accuracy\n",
    "preds = grid_search.predict(X_test)\n",
    "print('Accuracy on Test data is:', (accuracy_score(Y_test, preds))*100)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Test Data\")\n",
    "print(\"\\n\", classification_report(Y_test, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "clf = AdaBoostClassifier(random_state = 11, n_estimators = 100)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "# Pass the pipeline in to a cross_val_score and Output Training Accuracy \n",
    "scores = cross_val_score(pipe, data_X, data_Y, cv = 5)\n",
    "print(\"Accuracy with 5-fold cross validation on Training Data:\", scores.mean() * 100)\n",
    "\n",
    "# Fit the model to the Training Data\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Print Accuracy, Confusion matrix and Classification report for Test Data\n",
    "pred_Y = cross_val_predict(clf_rf, X_test, Y_test, cv = 10)\n",
    "print(\"Accuracy with 10-fold cross validation on Test Data:\",accuracy_score(Y_test, pred_Y)*100)\n",
    "print()\n",
    "print('The confusion matrix is:\\n', confusion_matrix(Y_test, pred_Y))\n",
    "print('\\n', classification_report(Y_test, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "clf = GradientBoostingClassifier(random_state = 11, n_estimators = 100)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "# Pass the pipeline in to a cross_val_score and Output Training Accuracy \n",
    "scores = cross_val_score(pipe, data_X, data_Y, cv = 5)\n",
    "print(\"Accuracy with 5-fold cross validation on Training Data:\", scores.mean() * 100)\n",
    "\n",
    "# Fit the model to the Training Data\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Print Accuracy, Confusion matrix and Classification report for Test Data\n",
    "pred_Y = cross_val_predict(clf_rf, X_test, Y_test, cv = 10)\n",
    "print(\"Accuracy with 10-fold cross validation on Test Data:\",accuracy_score(Y_test, pred_Y)*100)\n",
    "print()\n",
    "print('The confusion matrix is:\\n', confusion_matrix(Y_test, pred_Y))\n",
    "print('\\n', classification_report(Y_test, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pca = PCA(0.95, random_state = 11)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('pca', pca), ('knn', knn)])\n",
    "\n",
    "# Set parameters\n",
    "params = {'knn__n_neighbors': [3, 4, 5]}\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(pipe, params, cv = 10)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy on Train Data is :\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print Test Accuracy\n",
    "preds = grid_search.predict(X_test)\n",
    "print('Accuracy on Test data is:', (accuracy_score(Y_test, preds))*100)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Test Data\")\n",
    "print(\"\\n\", classification_report(Y_test, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Creating svc. Here we use the default instead of linear as mentioned on the piazza post. \n",
    "svc_clf = SVC(random_state = 11)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(steps = [('sca', scaler ), ('svc', svc_clf)])\n",
    "\n",
    "# Pass the pipeline in to a cross_val_score and Output Training Accuracy \n",
    "scores = cross_val_score(pipe, data_X, data_Y, cv = 5)\n",
    "print(\"Accuracy with 5-fold cross validation on Training Data:\", scores.mean() * 100)\n",
    "\n",
    "\n",
    "# Fit the model to the Training Data\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Print Accuracy, Confusion matrix and Classification report for Test Data\n",
    "pred_Y = cross_val_predict(clf_rf, X_test, Y_test, cv = 10)\n",
    "print(\"Accuracy with 10-fold cross validation on Test Data:\",accuracy_score(Y_test, pred_Y)*100)\n",
    "print()\n",
    "print('The confusion matrix is:\\n', confusion_matrix(Y_test, pred_Y))\n",
    "print('\\n', classification_report(Y_test, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 10-fold cross validation on Training Data: 57.64216334063086\n",
      "Accuracy with 10-fold cross validation on Test Data: 59.20669577874818\n",
      "\n",
      "The confusion matrix is:\n",
      " [[ 330 1029]\n",
      " [  92 1297]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.24      0.37      1359\n",
      "           1       0.56      0.93      0.70      1389\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      2748\n",
      "   macro avg       0.67      0.59      0.53      2748\n",
      "weighted avg       0.67      0.59      0.54      2748\n",
      "\n",
      "CPU times: user 1.07 s, sys: 179 ms, total: 1.25 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create NB clf and fit it\n",
    "clf_rf = GaussianNB()\n",
    "\n",
    "# Create Train and Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2)\n",
    "\n",
    "# Output Accuracy of Training Data\n",
    "scores = cross_val_score(clf_rf, X_train, Y_train, cv = 10)  \n",
    "print(\"Accuracy with 10-fold cross validation on Training Data:\", scores.mean() * 100)\n",
    "\n",
    "# Fit the model to the Training Data\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Print Accuracy, Confusion matrix and Classification report for Test Data\n",
    "pred_Y = cross_val_predict(clf_rf, X_test, Y_test, cv = 10)\n",
    "print(\"Accuracy with 10-fold cross validation on Test Data:\",accuracy_score(Y_test, pred_Y)*100)\n",
    "print()\n",
    "print('The confusion matrix is:\\n', confusion_matrix(Y_test, pred_Y))\n",
    "print('\\n', classification_report(Y_test, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = MLPClassifier(random_state = 11)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline(steps = [('scaler', scaler), ('clf', clf)])\n",
    "\n",
    "# Set parameters\n",
    "param_grid = {\n",
    "    'clf__hidden_layer_sizes': [(50,), (60,), (70,), (80,), (90,), (100,)],\n",
    "    'clf__activation': ['identity', 'logistic', 'tanh', 'relu']\n",
    "}\n",
    "\n",
    "# Find best parameters\n",
    "grid_search = GridSearchCV(pipe, param_grid, iid = False, cv = 5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(grid_search.best_params_)\n",
    "print(\"Accuracy on Train Data is :\", grid_search.best_score_ * 100)\n",
    "\n",
    "# Print Test Accuracy\n",
    "preds = grid_search.predict(X_test)\n",
    "print('Accuracy on Test data is:', (accuracy_score(Y_test, preds))*100)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Test Data\")\n",
    "print(\"\\n\", classification_report(Y_test, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('EMMIECHNG@UTEXAS.EDU', 'QMCZgepW6u')\n",
    "\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = 'Speed_Dating_Clean_SMOTE_train.csv'\n",
    "dataset_test = 'Speed_Dating_Clean_SMOTE_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload train dataset to Darwin\n",
    "data_SMOTE_train = pd.read_csv(\"Speed_Dating_Clean_SMOTE_train.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_SMOTE_train.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test dataset to Darwin\n",
    "data_SMOTE_test = pd.read_csv(\"Speed_Dating_Clean_SMOTE_test.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_SMOTE_test.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_train, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean test dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_test, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Darwin model\n",
    "model = target + \"_model\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = dataset_train,\n",
    "                                 model_name = model,\n",
    "                                 max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra training for Darwin model\n",
    "status, job_id = ds.resume_training_model(dataset_names = dataset_train, \n",
    "                                         model_name = model,\n",
    "                                         max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model decided by Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Darwin used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.display_population(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze DeepNeuralNetwork model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin DeepNeuralNetwork model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze RandomForest model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin RandomForest model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze GradientBoosted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin GradientBoosted model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_SMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_SMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_SMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin (noSMOTE) : imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('EMMIECHNG@UTEXAS.EDU', 'QMCZgepW6u')\n",
    "\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = 'Speed_Dating_Clean_noSMOTE_train.csv'\n",
    "dataset_test = 'Speed_Dating_Clean_noSMOTE_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload train dataset to Darwin\n",
    "data_noSMOTE_train = pd.read_csv(\"Speed_Dating_Clean_noSMOTE_train.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_noSMOTE_train.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test dataset to Darwin\n",
    "data_noSMOTE_test = pd.read_csv(\"Speed_Dating_Clean_noSMOTE_test.csv\")\n",
    "status, dataset = ds.upload_dataset(\"Speed_Dating_Clean_noSMOTE_test.csv\")\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_train, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean test dataset on Darwin\n",
    "target = \"match\"\n",
    "status, job_id = ds.clean_data(dataset_test, target = target)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Darwin model\n",
    "model = target + \"_model_noSMOTE\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = dataset_train,\n",
    "                                 model_name = model,\n",
    "                                 max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra training for Darwin model\n",
    "status, job_id = ds.resume_training_model(dataset_names = dataset_train, \n",
    "                                         model_name = model,\n",
    "                                         max_train_time = '00:05')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model decided by Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Darwin used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.display_population(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze DeepNeuralNetwork model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin DeepNeuralNetwork model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'DeepNeuralNetwork')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze RandomForest model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin RandomForest model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'RandomForest')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze GradientBoosted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Darwin GradientBoosted model\n",
    "status, artifact = ds.analyze_model(model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_train, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_train[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_train[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_train[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(dataset_test, model, model_type = 'GradientBoosted')\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])\n",
    "\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "print(\"\\n\", prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create plots comparing predictions with actual target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_noSMOTE_test[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(data_noSMOTE_test[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(data_noSMOTE_test[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(\"\\n\", classification_report(data_noSMOTE_test[target], prediction[target]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
